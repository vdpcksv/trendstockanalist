# -*- coding: utf-8 -*-
from fastapi import FastAPI, Request
from fastapi.responses import HTMLResponse, PlainTextResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
import json
import random
import FinanceDataReader as fdr
from contextlib import asynccontextmanager
from functools import lru_cache
from apscheduler.schedulers.background import BackgroundScheduler
from sqlalchemy.orm import Session
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordRequestForm
from fastapi.middleware.cors import CORSMiddleware

from fastapi_cache import FastAPICache
from fastapi_cache.backends.inmemory import InMemoryBackend
from fastapi_cache.decorator import cache

from database import engine, get_db
import models
import schemas
import auth
import ai_module # Phase 1: AI & ML Integration
import infra_module # Phase 3: External API & Telegram

# Create all tables in the database (this is safe if they already exist)
models.Base.metadata.create_all(bind=engine)

# --- Global Cache ---
# Stores the results of slow web scraping tasks to serve instantly
cache_data = {
    "money_flow": [],
    "theme_list": pd.DataFrame(),
    "prophet_models": {}, # {ticker: forecast_data_list}
    "llm_sentiment": {}   # {ticker: {"data": dict, "updated_at": datetime}}
}



HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

def get_money_flow_data():
    """Npay ì¦ê¶Œ êµ­ë‚´ì¦ì‹œ ë©”ì¸ í˜ì´ì§€ì—ì„œ íˆ¬ììë³„ ë™í–¥ì„ íŒŒì‹±í•´ì˜µë‹ˆë‹¤."""
    url = "https://finance.naver.com/sise/"
    try:
        response = requests.get(url, headers=HEADERS, timeout=5)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # íˆ¬ììë³„ ë§¤ë§¤ë™í–¥ íŒŒì‹± ë¡œì§ (ê¸°ì¡´ app.py ì°¸ê³ )
        flow_table = soup.select_one("div.box_type_m iframe") 
        if not flow_table:
            # ê¸°ë³¸ ëª¨ì˜ ë°ì´í„° ë¦¬í„´ (í¬ë¡¤ë§ ì‹¤íŒ¨ ì‹œ)
            return _get_mock_flow_data()
            
        # ì •í™•í•œ iframe srcë¥¼ ì¶”ì í•˜ê±°ë‚˜ ë”ë¯¸ë°ì´í„° ë°˜í™˜
        return _get_mock_flow_data()
    except Exception as e:
        print(f"Flow data error: {e}")
        return _get_mock_flow_data()

def _get_mock_flow_data():
    dates = []
    current_date = datetime.now()
    while len(dates) < 5:
        if current_date.weekday() < 5:  # ì›”~ê¸ˆ
            dates.append(current_date.strftime("%Y-%m-%d"))
        current_date -= timedelta(days=1)
        
    flow_data = []
    for d in dates:
        random.seed(d) # ë‚ ì§œë¥¼ ì‹œë“œë¡œ ì£¼ì–´ í•´ë‹¹ ë‚ ì§œì˜ ë°ì´í„°ëŠ” ê³ ì •ë˜ê²Œ í•¨
        flow_data.append({
            "Date": d,
            "ê°œì¸": random.randint(-2000, 2000),
            "ì™¸êµ­ì¸": random.randint(-1500, 2500),
            "ê¸°ê´€": random.randint(-1000, 1500)
        })
    
    # ë‹¤ë¥¸ ê³³ì˜ ëœë¤ì— ì˜í–¥ ì—†ë„ë¡ ì‹œë“œ ì´ˆê¸°í™”
    random.seed()
    
    return flow_data

# --- Background Task Functions ---
def fetch_and_cache_data():
    """Background task that periodically fetches scraping data."""
    try:
        print(f"[{datetime.now()}] Fetching background data...")
        flow_data = get_money_flow_data()
        theme_data = get_theme_list()
        
        # Safe update of cache
        if flow_data:
            cache_data["money_flow"] = flow_data
        if not theme_data.empty:
            cache_data["theme_list"] = theme_data
            
        print(f"[{datetime.now()}] Data fetch complete. Cached {len(flow_data)} flow records and {len(theme_data)} themes.")
    except Exception as e:
        print(f"Background fetch error: {e}")

def train_major_models():
    """Nightly background task to train Prophet models for major tickers."""
    major_tickers = ["005930", "000660", "373220"] # ì‚¼ì„±ì „ì, SKí•˜ì´ë‹‰ìŠ¤, LGì—ë„ˆì§€ì†”ë£¨ì…˜ ë“±
    print(f"[{datetime.now()}] Starting nightly AI model training...")
    try:
        end_date = datetime.now()
        start_date = end_date - pd.DateOffset(years=3) # í•™ìŠµìš© 3ë…„ì¹˜ ë°ì´í„°
        for ticker in major_tickers:
            df = fdr.DataReader(ticker, start_date, end_date)
            if not df.empty:
                forecast = ai_module.train_prophet_model(ticker, df)
                if forecast:
                    cache_data["prophet_models"][ticker] = forecast
        print(f"[{datetime.now()}] AI model training complete. Cached {len(cache_data['prophet_models'])} models.")
    except Exception as e:
        print(f"Background AI training error: {e}")

def calculate_mock_returns():
    """Phase 2: Nightly background task to calculate mock investment returns for users."""
    print(f"[{datetime.now()}] Starting mock investment settlement...")
    db = next(get_db())
    try:
        users = db.query(models.User).all()
        for user in users:
            total_value = 0.0
            portfolios = db.query(models.Portfolio).filter(models.Portfolio.user_id == user.id).all()
            for p in portfolios:
                if not p.target_price or p.target_price <= 0:
                    continue
                try:
                    df = fdr.DataReader(p.ticker, (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d'))
                    if not df.empty:
                        current_price = float(df['Close'].iloc[-1])
                        # Calculate return percentage for this stock
                        ret = (current_price - p.target_price) / p.target_price * 100
                        # Add to user's total return value (simple summation for MVP)
                        total_value += float(ret * p.qty)
                except Exception:
                    pass
            
            user.total_return = round(float(total_value), 2)
        
        db.commit()
        print(f"[{datetime.now()}] Mock investment settlement complete for {len(users)} users.")
    except Exception as e:
        db.rollback()
        print(f"Background settlement error: {e}")
    finally:
        db.close()

def process_alerts():
    """Phase 3: Background daemon to check live prices and send Telegram alerts."""
    print(f"[{datetime.now()}] Checking live prices for active alerts...")
    db = next(get_db())
    kis_api = infra_module.KisApiHandler()
    
    try:
        active_alerts = db.query(models.Alert).filter(models.Alert.is_active == 1).all()
        if not active_alerts:
            return
            
        checked_tickers = {} # Cache prices within this run
        
        for alert in active_alerts:
            user = db.query(models.User).filter(models.User.id == alert.user_id).first()
            if not user:
                continue
                
            current_price = checked_tickers.get(alert.ticker)
            
            if current_price is None:
                # 1. Try KIS API First
                try:
                    # current_price = kis_api.get_current_price(alert.ticker) # In Prod
                    raise Exception("Mocking KIS Failure to force Fallback")
                except Exception as e:
                    # 2. Fallback to scraping/FDR
                    try:
                        df = fdr.DataReader(alert.ticker, (datetime.now() - timedelta(days=5)).strftime('%Y-%m-%d'))
                        if not df.empty:
                            current_price = float(df['Close'].iloc[-1])
                    except:
                        continue # Skip if entirely failed
                        
                checked_tickers[alert.ticker] = current_price
                
            if current_price is None:
                continue
                
            # Check conditions
            triggered = False
            if alert.condition_type == 'ABOVE' and current_price >= alert.target_price:
                triggered = True
            elif alert.condition_type == 'BELOW' and current_price <= alert.target_price:
                triggered = True
                
            if triggered:
                # Send alert!
                msg = f"ğŸ”” [AlphaFinder ì•Œë¦¼]\n{user.username}ë‹˜, [{alert.ticker}] ì¢…ëª©ì´ ëª©í‘œê°€ {alert.target_price:,.0f}ì›ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤! (í˜„ì¬ê°€: {current_price:,.0f}ì›)"
                infra_module.send_telegram_sync(msg)
                
                # Mark as inactive to avoid spam
                alert.is_active = 0
                
        db.commit()
    except Exception as e:
        db.rollback()
        print(f"Alert processing error: {e}")
    finally:
        db.close()

# --- Application Lifespan Events ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Initialize cache backend
    FastAPICache.init(InMemoryBackend(), prefix="trendlotto-cache")
    
    # Startup: Initialize scheduler and run immediately
    scheduler = BackgroundScheduler()
    # Execute immediately on boot
    fetch_and_cache_data() 
    # Schedule to run every 10 minutes
    scheduler.add_job(fetch_and_cache_data, 'interval', minutes=10)
    
    # Phase 1: Schedule Nightly Prophet Model Training (e.g., at 2:00 AM)
    scheduler.add_job(train_major_models, 'cron', hour=2, minute=0)
    
    # Phase 2: Schedule Nightly Mock Investment Settlement (Midnight)
    scheduler.add_job(calculate_mock_returns, 'cron', hour=0, minute=0)
    
    # Phase 3: Schedule Alert Processing Daemon (Every 5 minutes)
    scheduler.add_job(process_alerts, 'interval', minutes=5)
    
    scheduler.start()
    
    yield # Hand control back to FastAPI
    
    # Shutdown: Stop scheduler
    scheduler.shutdown()

app = FastAPI(title="AlphaFinder Invest", lifespan=lifespan)

# Phase 3: Security - CORS Configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], # In production, restrict to actual frontend domains
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["*"],
)

# Phase 5: Global Exception Handler
import logging
import traceback as tb

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("alphafinder")

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """Catch-all handler for unhandled exceptions."""
    logger.error(f"Unhandled error on {request.method} {request.url}: {exc}")
    logger.error(tb.format_exc())
    return HTMLResponse(
        content="<h1>500 Internal Server Error</h1><p>ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.</p>",
        status_code=500
    )

@app.middleware("http")
async def log_requests(request: Request, call_next):
    """Log all incoming requests for monitoring."""
    start = datetime.now()
    response = await call_next(request)
    duration = (datetime.now() - start).total_seconds()
    if duration > 2.0:  # Only log slow requests (>2s)
        logger.warning(f"SLOW: {request.method} {request.url.path} took {duration:.2f}s")
    return response

# Serve static files (CSS, JS) securely mapped to /static
app.mount("/static", StaticFiles(directory="static"), name="static")

# Initialize Jinja2 templates directory
templates = Jinja2Templates(directory="templates")

@app.get("/", response_class=HTMLResponse)
async def read_dashboard(request: Request):
    # Use cached data instead of real-time scraping
    flow_data = cache_data.get("money_flow", [])
    if not flow_data:
        flow_data = _get_mock_flow_data()
    
    # Generate Insight
    last_record = flow_data[0]
    last_foreign = last_record["ì™¸êµ­ì¸"]
    last_instit = last_record["ê¸°ê´€"]
    
    if last_foreign > 0 and last_instit > 0:
        insight = f"ìµœê·¼ ì˜ì—…ì¼ ê¸°ì¤€ ì™¸êµ­ì¸({last_foreign}ì–µ)ê³¼ ê¸°ê´€({last_instit}ì–µ)ì´ ì–‘ë§¤ìˆ˜ë¥¼ ê¸°ë¡í•˜ë©° ìš°í˜¸ì ì¸ ì‹œì¥ í™˜ê²½ì´ ì¡°ì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
    elif last_foreign > 0:
        insight = f"ê¸°ê´€ì€ ë§¤ë„ ìš°ìœ„ì´ë‚˜, ì™¸êµ­ì¸ì´ {last_foreign}ì–µ ì› ìˆœë§¤ìˆ˜í•˜ë©° ì§€ìˆ˜ë¥¼ ë°©ì–´í•˜ê³  ìˆìŠµë‹ˆë‹¤."
    elif last_instit > 0:
        insight = f"ì™¸êµ­ì¸ì€ ë§¤ë„ ìš°ìœ„ì´ë‚˜, ê¸°ê´€ì´ {last_instit}ì–µ ì› ìˆœë§¤ìˆ˜í•˜ë©° ì‹œì¥ì„ ì´ëŒê³  ìˆìŠµë‹ˆë‹¤."
    else:
        insight = "í˜„ì¬ ê¸°ê´€ê³¼ ì™¸êµ­ì¸ ëª¨ë‘ ì–‘ë§¤ë„ë¥¼ ê¸°ë¡ ì¤‘ì…ë‹ˆë‹¤. ìˆ˜ê¸‰ ë³´ìˆ˜ì  ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤."
        
    return templates.TemplateResponse(
        request=request, name="dashboard.html",
        context={
            "flow_data_json": json.dumps(flow_data),
            "flow_data": flow_data,
            "insight": insight
        }
    )

def get_seasonality_data():
    """ëŒ€í‘œ ì„¹í„°ë³„ ìµœê·¼ 10ë…„ê°„ì˜ ì›”ë³„ ìŠ¹ë¥  ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    # (ì‹¤ ì„œë²„ í™˜ê²½ì—ì„œëŠ” fdrì„ í™œìš©í•´ ì‹¤ì‹œê°„ ì—°ì‚°í•˜ì§€ë§Œ, ì—¬ê¸°ì„  Prototype ì†ë„ë¥¼ ìœ„í•´ Mock Data ì‚¬ìš©)
    return {
        "ë°˜ë„ì²´": [60, 50, 40, 70, 55, 45, 65, 80, 50, 60, 70, 85],
        "2ì°¨ì „ì§€": [70, 60, 50, 45, 80, 75, 55, 60, 45, 50, 65, 90],
        "ë°”ì´ì˜¤": [40, 45, 55, 60, 50, 65, 70, 45, 80, 75, 60, 55],
        "ê¸ˆìœµ": [55, 60, 70, 80, 75, 65, 50, 45, 40, 50, 60, 65],
        "ìë™ì°¨": [50, 55, 65, 70, 60, 50, 45, 55, 65, 80, 75, 70],
        "ê²Œì„/ì—”í„°": [45, 50, 55, 60, 70, 80, 85, 75, 65, 55, 50, 45]
    }

@app.get("/seasonality", response_class=HTMLResponse)
async def read_seasonality(request: Request):
    season_data = get_seasonality_data()
    # DataFrameìœ¼ë¡œ ë³€í™˜ í›„ Heatmapìš© Z(ìŠ¹ë¥ ), X(ì›”), Y(ì„¹í„°) ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
    df_hm = pd.DataFrame(season_data).T 
    df_hm.columns = [f"{i}ì›”" for i in range(1, 13)]
    
    z_data = df_hm.values.tolist()
    y_labels = df_hm.index.tolist()
    x_labels = df_hm.columns.tolist()
    
    current_month_idx = datetime.now().month - 1
    
    return templates.TemplateResponse(
        request=request, name="seasonality.html",
        context={
            "z_data": json.dumps(z_data),
            "x_labels": json.dumps(x_labels),
            "y_labels": json.dumps(y_labels),
            "current_month_idx": current_month_idx
        }
    )

def get_theme_list():
    """Npay ì¦ê¶Œ êµ­ë‚´ì¦ì‹œ í…Œë§ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•´ì˜µë‹ˆë‹¤."""
    url = "https://finance.naver.com/sise/theme.naver"
    try:
        response = requests.get(url, headers=HEADERS, timeout=5)
        response.raise_for_status()
        response.encoding = 'cp949'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        themes = []
        table = soup.select_one('.type_1.theme')
        if not table:
            return pd.DataFrame()
            
        for tr in table.select('tr'):
            col_name = tr.select_one('td.col_type1 a')
            col_rate = tr.select_one('td.col_type2')
            
            if col_name and col_rate:
                link = "https://finance.naver.com" + col_name['href']
                themes.append({
                    "í…Œë§ˆëª…": col_name.text.strip(),
                    "ë“±ë½ë¥ (%)": col_rate.text.strip().replace('%', ''),
                    "ë§í¬": link
                })
        
        return pd.DataFrame(themes).head(20) if themes else pd.DataFrame()
    except Exception as e:
        print(f"Theme parsing error: {e}")
        return pd.DataFrame()

def get_theme_top_stocks(theme_url):
    """í•´ë‹¹ í…Œë§ˆ í˜ì´ì§€ ë‚´ ìƒìœ„ ë“±ë½ë¥  ì¢…ëª© 5ê°œë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
    try:
        response = requests.get(theme_url, headers=HEADERS, timeout=5)
        response.raise_for_status()
        response.encoding = 'cp949'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        stocks = []
        # í…Œë§ˆ í˜ì´ì§€ ë‚´ í¸ì… ì¢…ëª© í…Œì´ë¸”
        table = soup.select_one('.type_5 tbody')
        if not table:
            return pd.DataFrame()
            
        for idx, tr in enumerate(table.select('tr')):
            if idx > 10: break # ìµœëŒ€ 10ê°œë§Œ íƒìƒ‰ (ìƒìœ„ 5ê°œë¥¼ ìœ„í•´ ì—¬ìœ ë¶„ í™•ë³´)
            name_tag = tr.select_one('.name a')
            price_tag = tr.select_one('.number')
            rate_tag = tr.select('.number')
            
            if name_tag and price_tag and len(rate_tag) >= 3:
                # rate_tag êµ¬ì¡°: í˜„ì¬ê°€, ì „ì¼ë¹„, ë“±ë½ë¥  ...
                stocks.append({
                    "ì¢…ëª©ëª…": name_tag.text.strip(),
                    "í˜„ì¬ê°€": price_tag.text.strip(),
                    "ë“±ë½ë¥ ": rate_tag[2].text.strip().replace('\n', '').replace('\t', '')
                })
                if len(stocks) >= 5:
                    break
                    
        return pd.DataFrame(stocks)
    except Exception as e:
        print(f"Detailed Theme parsing error: {e}")
        return pd.DataFrame()

@app.get("/themes", response_class=HTMLResponse)
async def read_themes(request: Request, theme: str = None):
    # Use cached theme list
    df_themes = cache_data.get("theme_list", pd.DataFrame())
    if df_themes.empty:
        context = {"error": "í…Œë§ˆ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}
        return templates.TemplateResponse(request=request, name="themes.html", context=context)
    
    themes_data = df_themes.to_dict('records')
    context = {"themes": themes_data, "selected_theme_data": None, "stocks_data": None, "ai_comment": None}
    
    # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ ì„ íƒëœ í…Œë§ˆì˜ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
    if theme:
        selected_row = df_themes[df_themes['í…Œë§ˆëª…'] == theme]
        if not selected_row.empty:
            selected_info = selected_row.iloc[0]
            context["selected_theme_data"] = selected_info.to_dict()
            
            # ì£¼ë„ ì¢…ëª© ì¶”ì¶œ
            df_stocks = get_theme_top_stocks(selected_info['ë§í¬'])
            if not df_stocks.empty:
                context["stocks_data"] = df_stocks.to_dict('records')
                
            # AI ì‹œë‚˜ë¦¬ì˜¤ ì§„ë‹¨ ë¡œì§
            try:
                rate_val = float(selected_info['ë“±ë½ë¥ (%)'].replace('+', ''))
                if rate_val > 3.0:
                    ai_title = "ğŸ“ˆ ë§¤ìš° ê°•í•œ ìê¸ˆ ìœ ì…"
                    ai_desc = "í˜„ì¬ ì‹œì¥ ì£¼ë„ í…Œë§ˆë¡œ ì„ ì •ë˜ì—ˆìŠµë‹ˆë‹¤. ëŒ€ì¥ì£¼ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ì§§ì€ ë‹¨ê¸° íŠ¸ë ˆì´ë”© ì ‘ê·¼ì´ ìœ íš¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                elif rate_val > 0:
                    ai_title = "âš–ï¸ ì™„ë§Œí•œ ìƒìŠ¹ì„¸"
                    ai_desc = "ì¡°ìš©íˆ ìš°ìƒí–¥ ì¤‘ì¸ í…Œë§ˆì…ë‹ˆë‹¤. í–¥í›„ ëª¨ë©˜í…€(ë‰´ìŠ¤/ì •ì±…) ë°œìƒ ì‹œ ì¶”ê°€ ìŠˆíŒ…ì˜ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤."
                else:
                    ai_title = "ğŸ“‰ ì¡°ì • ì¤‘ (ëˆŒë¦¼ëª©)"
                    ai_desc = "í˜„ì¬ ë§¤ìˆ˜ì„¸ê°€ ì•½í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¨ê¸° ê¸‰ë½ í›„ ê³„ì ˆì  ë°˜ë“±ì„ ë…¸ë¦¬ëŠ” ì¤‘ê¸° ê´€ì ì˜ ë¶„í•  ë§¤ìˆ˜ ëª¨ë‹ˆí„°ë§ì´ í•„ìš”í•©ë‹ˆë‹¤."
                
                context["ai_comment"] = {"title": ai_title, "desc": ai_desc}
            except:
                pass

    return templates.TemplateResponse(request=request, name="themes.html", context=context)

def calculate_technical_indicators(df):
    """(ê¸°ì¡´ app.py ë¡œì§) ë‹¨ìˆœ ì´ë™í‰ê· , ë³¼ë¦°ì € ë°´ë“œ, RSI ê³„ì‚°"""
    df = df.copy()
    # 5/20/60ì¼ ì´ë™í‰ê· 
    df['MA5'] = df['Close'].rolling(window=5).mean()
    df['MA20'] = df['Close'].rolling(window=20).mean()
    df['MA60'] = df['Close'].rolling(window=60).mean()

    # ë³¼ë¦°ì € ë°´ë“œ (20ì¼, 2 standard deviations)
    df['BB_MB'] = df['MA20']
    df['BB_STD'] = df['Close'].rolling(window=20).std()
    df['BB_UPPER'] = df['BB_MB'] + (df['BB_STD'] * 2)
    df['BB_LOWER'] = df['BB_MB'] - (df['BB_STD'] * 2)

    # ê·¹ë‹¨ì  ë³¼ë¦°ì € ë°´ë“œ (20ì¼, 3 standard deviations)
    df['BB_UPPER_EXT'] = df['BB_MB'] + (df['BB_STD'] * 3)
    df['BB_LOWER_EXT'] = df['BB_MB'] - (df['BB_STD'] * 3)

    # RSI (14ì¼)
    delta = df['Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['RSI'] = 100 - (100 / (1 + rs))

    return df

@lru_cache(maxsize=1)
def get_krx_stock_listing():
    return fdr.StockListing('KRX-DESC')

def resolve_ticker(query: str):
    query = query.strip()
    if query.isdigit() and len(query) == 6:
        return query
    
    try:
        df = get_krx_stock_listing()
        matches = df[df['Name'] == query]
        if not matches.empty:
            return matches.iloc[0]['Code']
    except Exception as e:
        print(f"Error resolving ticker: {e}")
    return query

def get_stock_fundamentals(ticker: str):
    """Scrapes essential fundamental data using Naver mobile JSON API for stability."""
    url = f"https://m.stock.naver.com/api/stock/{ticker}/finance/annual"
    try:
        res = requests.get(url, headers=HEADERS, timeout=5)
        res.raise_for_status()
        data = res.json()
        
        headers = [item['title'] for item in data['financeInfo']['trTitleList']]
        parsed_data = {}
        
        target_indices = {
            0: "ë§¤ì¶œì•¡",
            1: "ì˜ì—…ì´ìµ",
            2: "ë‹¹ê¸°ìˆœì´ìµ",
            8: "ë¶€ì±„ë¹„ìœ¨",
            7: "ROE(ì§€ë°°ì£¼ì£¼)",
            12: "PER(ë°°)",
            14: "PBR(ë°°)"
        }
        
        row_list = data['financeInfo']['rowList']
        header_keys = [item['key'] for item in data['financeInfo']['trTitleList']]
        
        for idx, key_name in target_indices.items():
            if idx < len(row_list):
                row = row_list[idx]
                vals = []
                for hk in header_keys:
                    vals.append(row['columns'].get(hk, {}).get('value', '-'))
                parsed_data[key_name] = vals
                
        return {"headers": headers, "data": parsed_data}
    except Exception as e:
        print(f"Error fetching fundamentals: {e}")
        return None

def get_news_sentiment(ticker: str):
    """Fetches recent news from Naver Mobile API and performs keyword-based sentiment analysis."""
    url = f"https://m.stock.naver.com/api/news/stock/{ticker}?pageSize=15"
    
    pos_keywords = ['ìƒìŠ¹', 'ê¸‰ë“±', 'ëŒíŒŒ', 'í‘ì', 'ìˆ˜ì£¼', 'í˜¸ì¡°', 'MOU', 'ê°•ì„¸', 'ì²´ê²°', 'ìµœëŒ€', 'ì‹ ê³ ê°€', 'ì„±ì¥', 'ê¸°ëŒ€', 'ìˆ˜í˜œ', 'ë°˜ë“±']
    neg_keywords = ['í•˜ë½', 'ê¸‰ë½', 'ì ì', 'ìš°ë ¤', 'ìˆ˜ì‚¬', 'ì•…ì¬', 'ì•½ì„¸', 'ì‹ ì €ê°€', 'ë¯¸ë‹¬', 'ì‡¼í¬', 'ë§¤ë„', 'ë¶ˆì•ˆ', 'ìœ„ê¸°', 'ë¦¬ìŠ¤í¬']
    
    try:
        res = requests.get(url, headers=HEADERS, timeout=5)
        res.raise_for_status()
        data = json.loads(res.content.decode('utf-8', 'ignore'))
        
        headlines = []
        for group in data:
            for item in group.get('items', []):
                title = item.get('title', '')
                if title:
                    title = title.replace('&quot;', '"').replace('&lt;', '<').replace('&gt;', '>')
                    headlines.append(title)
                    if len(headlines) >= 15:
                        break
            if len(headlines) >= 15:
                break
                
        pos_count = 0
        neg_count = 0
        neutral_count = 0
        analyzed_news = []
        
        for title in headlines:
            is_pos = any(kw in title for kw in pos_keywords)
            is_neg = any(kw in title for kw in neg_keywords)
            
            if is_pos and not is_neg:
                sentiment = 'positive'
                pos_count += 1
            elif is_neg and not is_pos:
                sentiment = 'negative'
                neg_count += 1
            else:
                sentiment = 'neutral'
                neutral_count += 1
                
            analyzed_news.append({"title": title, "sentiment": sentiment})
            
        total = len(headlines)
        if total == 0:
            return None
            
        # ê¸ì • -> ì¤‘ë¦½ -> ë¶€ì • ìˆœìœ¼ë¡œ ì •ë ¬
        sentiment_order = {'positive': 0, 'neutral': 1, 'negative': 2}
        analyzed_news.sort(key=lambda x: sentiment_order.get(x['sentiment'], 3))
            
        # Phase 1: LLM ê°ì„± ë¶„ì„ ì‹œë„ (ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë¡œì§ ê²°ê³¼ë§Œ ë°˜í™˜)
        llm_result = None
        # ìºì‹œ í™•ì¸ (1ì‹œê°„ ì´ë‚´)
        cached_llm = cache_data["llm_sentiment"].get(ticker)
        if cached_llm and (datetime.now() - cached_llm['updated_at']).total_seconds() < 3600:
            llm_result = cached_llm['data']
        else:
            # LLM ë¶„ì„ (ì‹œê°„ ì œí•œ í”¼í•˜ê¸° ìœ„í•´ headlineë§Œ ì „ë‹¬)
            llm_result = ai_module.analyze_news_sentiment_with_llm(ticker, [news['title'] for news in analyzed_news])
            if llm_result:
                cache_data["llm_sentiment"][ticker] = {
                    "data": llm_result,
                    "updated_at": datetime.now()
                }

        return {
            "total": total,
            "positive_ratio": round((pos_count / total) * 100),
            "negative_ratio": round((neg_count / total) * 100),
            "neutral_ratio": round((neutral_count / total) * 100),
            "pos_count": pos_count,
            "neg_count": neg_count,
            "neutral_count": neutral_count,
            "news_list": analyzed_news,
            "llm_analysis": llm_result # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ í™œìš©
        }
    except Exception as e:
        print(f"Error fetching news sentiment: {e}")
        return None

@app.get("/api/stock_seasonality/{ticker}")
@cache(expire=1800) # ìºì‹œ ìœ ì§€ì‹œê°„: 30ë¶„
async def get_stock_seasonality(ticker: str):
    """ìµœê·¼ 10ë…„ì¹˜ ì¼ë´‰ì„ ë°”íƒ•ìœ¼ë¡œ ì›”ë³„ ìŠ¹ë¥ ê³¼ í‰ê·  ìˆ˜ìµë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
    # ì¢…ëª©ëª… ì…ë ¥ì‹œ ë¡œì§ì— ì˜í•´ ì½”ë“œë¡œ ì¹˜í™˜
    actual_ticker = resolve_ticker(ticker)
    try:
        end_date = datetime.now()
        start_date = end_date - pd.DateOffset(years=10)
        
        df = fdr.DataReader(actual_ticker, start_date, end_date)
        if df.empty:
            raise HTTPException(status_code=404, detail="ì¢…ëª© ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        df['Year'] = df.index.year
        df['Month'] = df.index.month

        # ì›”ë³„ ì²« ì‹œê°€, ë§ˆì§€ë§‰ ì¢…ê°€ ì¶”ì¶œ
        monthly_data = df.groupby(['Year', 'Month']).agg(
            Open=('Open', 'first'),
            Close=('Close', 'last')
        ).reset_index()
        
        # ì›”ë³„ ìˆ˜ìµë¥  ê³„ì‚° (%) = (ì¢…ê°€-ì‹œê°€)/ì‹œê°€ * 100
        monthly_data['Return'] = (monthly_data['Close'] - monthly_data['Open']) / monthly_data['Open'] * 100
        
        seasonality = []
        for month in range(1, 13):
            month_data = monthly_data[monthly_data['Month'] == month]
            if month_data.empty:
                seasonality.append({"Month": month, "WinRate": 0, "AvgReturn": 0})
                continue
            
            total_years = len(month_data)
            win_years = len(month_data[month_data['Return'] > 0])
            
            win_rate = (win_years / total_years) * 100
            avg_return = month_data['Return'].mean()
            
            seasonality.append({
                "Month": month,
                "WinRate": round(win_rate, 1),
                "AvgReturn": round(avg_return, 2)
            })
            
        return {"status": "success", "data": seasonality}
        
    except Exception as e:
        print(f"Seasonality API Error: {e}")
        return {"status": "error", "message": str(e)}

@app.get("/leaderboard", response_class=HTMLResponse)
async def read_leaderboard(request: Request):
    """Phase 2: Render the mock investment leaderboard page."""
    return templates.TemplateResponse(request=request, name="leaderboard.html")

@app.get("/review", response_class=HTMLResponse)
async def read_review(request: Request, ticker: str = "005930"): # ê¸°ë³¸ê°’: ì‚¼ì„±ì „ì
    search_name = ticker.strip()
    actual_ticker = resolve_ticker(search_name)
    
    context = {"ticker": actual_ticker, "search_name": search_name, "error": None, "chart_data": None, "ai_score": None, "fundamentals": None, "sentiment_data": None}
    
    try:
        # ìµœê·¼ 6ê°œì›” ë°ì´í„° ë¡œë“œ
        end_date = datetime.now()
        start_date = end_date - pd.DateOffset(months=6)
        
        # DataFrame ë¡œì»¬ ë³€ìˆ˜
        df = fdr.DataReader(actual_ticker, start_date, end_date)
        if df.empty:
            context["error"] = "ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¢…ëª© ì½”ë“œë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”."
            return templates.TemplateResponse(request=request, name="review.html", context=context)
            
        df = calculate_technical_indicators(df)
        df = df.dropna() # ì§€í‘œ ê³„ì‚° í›„ NaN ì œê±°
        
        # ë°ì´í„° í”„ë ˆì  í…Œì´ì…˜ (ìˆ˜ì • ë¶ˆê°€)
        df.reset_index(inplace=True)
        if 'Date' in df.columns:
            df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')
            
        records = df.to_dict('records')
        context["chart_data"] = json.dumps(records)
        
        # Phase 1: Prophet í¬ìºìŠ¤íŠ¸ ê²°í•©
        forecast = cache_data["prophet_models"].get(actual_ticker)
        if not forecast:
            # ì£¼ìš” ì¢…ëª©ì´ ì•„ë‹ˆë©´ ì‹¤ì‹œê°„ìœ¼ë¡œ ì˜ˆì¸¡ (3ë…„ì¹˜ í•„ìš”)
            big_df = fdr.DataReader(actual_ticker, start_date - pd.DateOffset(years=2), end_date)
            forecast = ai_module.train_prophet_model(actual_ticker, big_df)
            if forecast:
                cache_data["prophet_models"][actual_ticker] = forecast
        
        context["prophet_forecast"] = json.dumps(forecast) if forecast else None

        # --- AI í€€íŠ¸ ì¢…í•© ë¶„ì„ (ë¡œì§ í¬íŒ…) ---
        last_row = df.iloc[-1]
        score = 50
        
        # ì´ë™í‰ê·  ì •ë°°ì—´/ì—­ë°°ì—´ ê°€ì 
        if last_row['MA5'] > last_row['MA20'] > last_row['MA60']: score += 20
        elif last_row['MA5'] < last_row['MA20'] < last_row['MA60']: score -= 20
            
        # RSI ì ìˆ˜ ë¡œì§ (ê³ ê¸‰)
        current_rsi = last_row['RSI']
        if current_rsi < 30: score += 15 # ê³¼ë§¤ë„ (ë°˜ë“± ê¸°ëŒ€)
        elif current_rsi > 70: score -= 15 # ê³¼ë§¤ìˆ˜ (ì¡°ì • ìš°ë ¤)
        elif 40 <= current_rsi <= 60: score += 5 # ì•ˆì •ì  ì¶”ì„¸
        
        # ë³¼ë¦°ì € ë°´ë“œ ìœ„ì¹˜ (3í‘œì¤€í¸ì°¨ í¬í•¨)
        current_price = last_row['Close']
        if current_price < last_row['BB_LOWER_EXT']: score += 25 # ê·¹ë‹¨ì  í•˜ë‹¨ ì´íƒˆ (ê°•í•œ ë¬¼íƒ€ê¸°/ë§¤ìˆ˜)
        elif current_price < last_row['BB_LOWER']: score += 10 # ë°´ë“œ í•˜ë‹¨ ì´íƒˆ (ë‹¨ê¸° ë°˜ë“±)
        elif current_price > last_row['BB_UPPER_EXT']: score -= 25 # ê·¹ë‹¨ì  ìƒë‹¨ ì´íƒˆ (ê°•í•œ ì°¨ìµì‹¤í˜„)
        elif current_price > last_row['BB_UPPER']: score -= 10 # ë°´ë“œ ìƒë‹¨ ëŒíŒŒ (ê³¼ì—´)
        elif current_price > last_row['MA5']: score += 5 # ë‹¨ê¸° ì´í‰ì„  ì§€ì§€
            
        # ì ìˆ˜ ì •ê·œí™” (0~100)
        final_score = max(0, min(100, score))
        
        # ì‹œë‚˜ë¦¬ì˜¤ ë§¤í•‘
        if final_score >= 80: phase_text = "ê·¹ë‹¨ì  ê³¼ë§¤ë„ (ê¸°ìˆ ì  ë°˜ë“± ê°€ëŠ¥ì„± êµ¬ê°„)"
        elif final_score >= 60: phase_text = "ìƒìŠ¹ ì¶”ì„¸ (í™€ë”© ë° ë¶„í•  ë§¤ìˆ˜)"
        elif final_score >= 40: phase_text = "ì¤‘ë¦½/ë°•ìŠ¤ê¶Œ (ê´€ë§)"
        elif final_score >= 20: phase_text = "í•˜ë½ ì¶”ì„¸ (ì‹ ê·œ ë§¤ìˆ˜ ë³´ë¥˜)"
        else: phase_text = "ê·¹ë‹¨ì  ê³¼ë§¤ìˆ˜ (í˜„ê¸ˆí™”/ìµì ˆ íƒ€ì )"
            
        context["ai_score"] = {
            "score": round(final_score),
            "phase": phase_text,
            "rsi": round(current_rsi, 2)
        }
        
        # í€ë”ë©˜í„¸ ë°ì´í„° ìˆ˜ì§‘ ê²°í•©
        context["fundamentals"] = get_stock_fundamentals(actual_ticker)
        
        # ë‰´ìŠ¤ ì„¼í‹°ë©˜íŠ¸ ë¶„ì„ ê²°í•©
        context["sentiment_data"] = get_news_sentiment(actual_ticker)
        
        return templates.TemplateResponse(request=request, name="review.html", context=context)
            
    except Exception as e:
        context["error"] = f"ì—ëŸ¬ ë°œìƒ: {e}"

    return templates.TemplateResponse(request=request, name="review.html", context=context)

# ---------------------------------------------------------
# Tab 5 & 6 equivalents: Portfolio and Alerts (Form Handlers)
# ---------------------------------------------------------

@app.get("/portfolio", response_class=HTMLResponse)
async def read_portfolio(request: Request):
    context = {"error": None}
    return templates.TemplateResponse(request=request, name="portfolio.html", context=context)
    
@app.get("/policies", response_class=HTMLResponse)
async def read_policies(request: Request):
    # Legal Policies and AdSense Guide
    return templates.TemplateResponse(request=request, name="policies.html", context={})

# --- API Endpoints for DB CRUD & Auth ---
@app.post("/api/register") # Removed response_model to prevent validation error when returning a dict
def register_user(user: schemas.UserCreate, db: Session = Depends(get_db)):
    db_user = db.query(models.User).filter(models.User.username == user.username).first()
    if db_user:
        raise HTTPException(status_code=400, detail="ì´ë¯¸ ë“±ë¡ëœ ì•„ì´ë””ì…ë‹ˆë‹¤.")
    
    hashed_password = auth.get_password_hash(user.password)
    # By default, we grant premium for testing. In prod, this is triggered by payment.
    db_user = models.User(username=user.username, hashed_password=hashed_password, membership="premium")
    db.add(db_user)
    db.commit()
    db.refresh(db_user)
    return {"message": "íšŒì›ê°€ì…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!"}

@app.post("/api/token")
def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends(), db: Session = Depends(get_db)):
    user = db.query(models.User).filter(models.User.username == form_data.username).first()
    if not user or not auth.verify_password(form_data.password, user.hashed_password):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="ì•„ì´ë”” ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.",
            headers={"WWW-Authenticate": "Bearer"},
        )
    access_token_expires = auth.timedelta(minutes=auth.ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = auth.create_access_token(
        data={"sub": user.username}, expires_delta=access_token_expires
    )
    return {"access_token": access_token, "token_type": "bearer", "membership": user.membership}

@app.post("/api/portfolio", response_model=schemas.Portfolio)
def add_portfolio_item(item: schemas.PortfolioCreate, db: Session = Depends(get_db), current_user: models.User = Depends(auth.get_current_user)):
    # Check if the user already has this ticker in their portfolio
    existing_item = db.query(models.Portfolio).filter(
        models.Portfolio.user_id == current_user.id,
        models.Portfolio.ticker == item.ticker
    ).first()

    if existing_item:
        # Calculate new average target (buy) price
        old_qty = existing_item.qty or 1
        old_price = existing_item.target_price or 0.0
        
        new_qty = item.qty or 1
        new_price = item.target_price or 0.0

        total_old_value = old_qty * old_price
        total_new_value = new_qty * new_price
        
        combined_qty = old_qty + new_qty
        avg_price = (total_old_value + total_new_value) / combined_qty

        # Update the existing record
        existing_item.qty = combined_qty
        existing_item.target_price = avg_price
        db.commit()
        db.refresh(existing_item)
        return existing_item
    else:
        # Create a new record
        db_item = models.Portfolio(
            ticker=item.ticker,
            target_price=item.target_price,
            qty=item.qty,
            user_id=current_user.id
        )
        db.add(db_item)
        db.commit()
        db.refresh(db_item)
        return db_item

def get_current_price(stock_name: str) -> float:
    try:
        ticker = resolve_ticker(stock_name)
        # Fetch data for the last 7 days to ensure we get the latest trading day
        start_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
        df = fdr.DataReader(ticker, start=start_date)
        if not df.empty:
            return float(df.iloc[-1]['Close'])
    except Exception as e:
        print(f"Error fetching current price for {stock_name}: {e}")
    return 0.0

@app.get("/api/portfolio")
def get_portfolio_items(db: Session = Depends(get_db), current_user: models.User = Depends(auth.get_current_user)):
    items = db.query(models.Portfolio).filter(models.Portfolio.user_id == current_user.id).all()
    
    result = []
    for i in items:
        # In case the table was created before qty column, fallback to 1
        qty = i.qty if hasattr(i, 'qty') and i.qty is not None else 1
        current_price = get_current_price(i.ticker)
        result.append({
            "id": i.id, 
            "name": i.ticker, 
            "price": i.target_price or 0, # This is the buy price
            "qty": qty,
            "current_price": current_price
        })
    return result

@app.delete("/api/portfolio/{item_id}")
def delete_portfolio_item(item_id: int, db: Session = Depends(get_db), current_user: models.User = Depends(auth.get_current_user)):
    item = db.query(models.Portfolio).filter(models.Portfolio.id == item_id, models.Portfolio.user_id == current_user.id).first()
    if item:
        db.delete(item)
        db.commit()
        return {"status": "success"}
    raise HTTPException(status_code=404, detail="í•´ë‹¹ í•­ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# --- Phase 2: Community API (Comments & Votes) ---
@app.post("/api/comments/{ticker}", response_model=schemas.CommentResponse)
def create_comment(ticker: str, comment: schemas.CommentCreate, db: Session = Depends(get_db), current_user: models.User = Depends(auth.get_current_user)):
    resolved_ticker = resolve_ticker(ticker)
    db_comment = models.Comment(
        user_id=current_user.id,
        ticker=resolved_ticker,
        content=comment.content
    )
    db.add(db_comment)
    db.commit()
    db.refresh(db_comment)
    
    return schemas.CommentResponse(
        id=db_comment.id,
        content=db_comment.content,
        ticker=db_comment.ticker,
        user_id=db_comment.user_id,
        created_at=db_comment.created_at,
        username=current_user.username
    )

@app.get("/api/comments/{ticker}")
def get_comments(ticker: str, db: Session = Depends(get_db)):
    resolved_ticker = resolve_ticker(ticker)
    comments = db.query(models.Comment, models.User.username)\
        .join(models.User, models.Comment.user_id == models.User.id)\
        .filter(models.Comment.ticker == resolved_ticker)\
        .order_by(models.Comment.created_at.desc())\
        .limit(50).all()
        
    result = []
    for c, uname in comments:
        result.append({
            "id": c.id,
            "content": c.content,
            "ticker": c.ticker,
            "user_id": c.user_id,
            "created_at": c.created_at.isoformat(),
            "username": uname
        })
    return result

@app.post("/api/votes/{ticker}")
def cast_vote(ticker: str, vote: schemas.VoteCreate, db: Session = Depends(get_db), current_user: models.User = Depends(auth.get_current_user)):
    resolved_ticker = resolve_ticker(ticker)
    existing_vote = db.query(models.Vote).filter(
        models.Vote.user_id == current_user.id,
        models.Vote.ticker == resolved_ticker
    ).first()
    
    if existing_vote:
        existing_vote.vote_type = vote.vote_type
    else:
        new_vote = models.Vote(
            user_id=current_user.id,
            ticker=resolved_ticker,
            vote_type=vote.vote_type
        )
        db.add(new_vote)
        
    db.commit()
    return {"status": "success"}

@app.get("/api/votes/{ticker}")
def get_votes(ticker: str, db: Session = Depends(get_db)):
    resolved_ticker = resolve_ticker(ticker)
    bull_count = db.query(models.Vote).filter(models.Vote.ticker == resolved_ticker, models.Vote.vote_type == 'BULL').count()
    bear_count = db.query(models.Vote).filter(models.Vote.ticker == resolved_ticker, models.Vote.vote_type == 'BEAR').count()
    total = bull_count + bear_count
    
    return {
        "bull": bull_count,
        "bear": bear_count,
        "total": total,
        "bull_ratio": round(bull_count / total * 100) if total > 0 else 0,
        "bear_ratio": round(bear_count / total * 100) if total > 0 else 0
    }

@app.get("/api/leaderboard")
def get_leaderboard(db: Session = Depends(get_db), limit: int = 10):
    """Phase 2: Fetch top users ranked by mock investment returns."""
    top_users = db.query(models.User.username, models.User.total_return)\
        .filter(models.User.total_return.isnot(None))\
        .order_by(models.User.total_return.desc())\
        .limit(limit).all()
        
    return [{"rank": i+1, "username": u.username, "return": u.total_return} for i, u in enumerate(top_users)]

# --- Phase 3: Alert CRUD API Endpoints ---
@app.post("/api/alerts", response_model=schemas.AlertResponse)
def create_alert(alert: schemas.AlertCreate, db: Session = Depends(get_db), current_user: models.User = Depends(auth.get_current_user)):
    """Phase 3: Create a price alert for a stock ticker."""
    # Validate condition_type
    if alert.condition_type not in ('ABOVE', 'BELOW'):
        raise HTTPException(status_code=400, detail="ì¡°ê±´ ìœ í˜•ì€ 'ABOVE' ë˜ëŠ” 'BELOW'ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    
    # Limit active alerts per user (prevent spam)
    active_count = db.query(models.Alert).filter(
        models.Alert.user_id == current_user.id,
        models.Alert.is_active == 1
    ).count()
    if active_count >= 10:
        raise HTTPException(status_code=400, detail="í™œì„± ì•Œë¦¼ì€ ìµœëŒ€ 10ê°œê¹Œì§€ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    resolved_ticker = resolve_ticker(alert.ticker)
    
    db_alert = models.Alert(
        user_id=current_user.id,
        ticker=resolved_ticker,
        target_price=alert.target_price,
        condition_type=alert.condition_type,
        is_active=1
    )
    db.add(db_alert)
    db.commit()
    db.refresh(db_alert)
    return db_alert

@app.get("/api/alerts")
def get_my_alerts(db: Session = Depends(get_db), current_user: models.User = Depends(auth.get_current_user)):
    """Phase 3: Get current user's active alerts."""
    alerts = db.query(models.Alert).filter(
        models.Alert.user_id == current_user.id
    ).order_by(models.Alert.created_at.desc()).all()
    
    return [{
        "id": a.id,
        "ticker": a.ticker,
        "target_price": a.target_price,
        "condition_type": a.condition_type,
        "is_active": a.is_active,
        "created_at": str(a.created_at)
    } for a in alerts]

@app.delete("/api/alerts/{alert_id}")
def delete_alert(alert_id: int, db: Session = Depends(get_db), current_user: models.User = Depends(auth.get_current_user)):
    """Phase 3: Delete a specific alert."""
    alert = db.query(models.Alert).filter(
        models.Alert.id == alert_id,
        models.Alert.user_id == current_user.id
    ).first()
    
    if not alert:
        raise HTTPException(status_code=404, detail="ì•Œë¦¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    db.delete(alert)
    db.commit()
    return {"status": "success", "message": "ì•Œë¦¼ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."}

# --- Google AdSense ads.txt ì¸ì¦ ìš°íšŒ ë¼ìš°íŠ¸ ---
@app.get("/ads.txt", response_class=PlainTextResponse)
async def get_ads_txt():
    # ìº¡ì²˜ í™”ë©´ì—ì„œ í™•ì¸í•œ ë³¸ì¸ì˜ pub IDë¥¼ ì ìš©í•œ ê³µì‹ ì¸ì¦ í…ìŠ¤íŠ¸
    return "google.com, pub-9065075656013134, DIRECT, f08c47fec0942fa0"

# =====================================================
# Phase 4: Monetization & Marketing Endpoints
# =====================================================

# --- 4-1. Freemium Membership API ---
@app.get("/api/membership")
def get_membership(current_user: models.User = Depends(auth.get_current_user)):
    """Check current user's membership status."""
    return {
        "username": current_user.username,
        "membership": current_user.membership or "basic",
        "features": {
            "ai_analysis": True,  # Available to all
            "community": True,    # Available to all
            "alerts": current_user.membership == "premium",
            "unlimited_alerts": current_user.membership == "premium",
            "priority_support": current_user.membership == "premium",
        }
    }

@app.post("/api/membership/upgrade")
def upgrade_membership(
    db: Session = Depends(get_db),
    current_user: models.User = Depends(auth.get_current_user)
):
    """
    Phase 4: Upgrade user to premium.
    In production, this would be called after payment confirmation webhook.
    For MVP, this is a direct upgrade endpoint.
    """
    current_user.membership = "premium"
    db.commit()
    return {"status": "success", "message": "í”„ë¦¬ë¯¸ì—„ íšŒì›ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œë˜ì—ˆìŠµë‹ˆë‹¤!", "membership": "premium"}

# --- 4-2. Payment Webhook (Toss Payments / PortOne Ready) ---
@app.post("/api/payment/confirm")
async def payment_confirm(request: Request, db: Session = Depends(get_db)):
    """
    Phase 4: Payment confirmation webhook receiver.
    In production, verify the payment with Toss/PortOne API before upgrading.
    """
    try:
        body = await request.json()
        payment_key = body.get("paymentKey", "")
        order_id = body.get("orderId", "")
        amount = body.get("amount", 0)
        
        # Validate required fields
        if not payment_key or not order_id or not amount:
            raise HTTPException(status_code=400, detail="í•„ìˆ˜ ê²°ì œ ì •ë³´(paymentKey, orderId, amount)ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # TODO: In production, verify payment with external API:
        # POST https://api.tosspayments.com/v1/payments/confirm
        # with paymentKey, orderId, amount
        
        # For MVP, log the payment attempt
        print(f"[Payment] Received: key={payment_key}, order={order_id}, amount={amount}")
        
        return {"status": "success", "message": "ê²°ì œ í™•ì¸ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"ê²°ì œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.post("/api/membership/downgrade")
def downgrade_membership(
    db: Session = Depends(get_db),
    current_user: models.User = Depends(auth.get_current_user)
):
    """Phase 4: Downgrade user back to basic (cancel premium)."""
    current_user.membership = "basic"
    db.commit()
    return {"status": "success", "message": "ê¸°ë³¸ íšŒì›ìœ¼ë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.", "membership": "basic"}

# --- 4-3. SEO: Dynamic Sitemap.xml & robots.txt ---
from fastapi.responses import Response

@app.get("/sitemap.xml")
async def sitemap():
    """Phase 4: Generate dynamic sitemap for SEO crawlers."""
    base_url = "https://alphafinder.kr"
    today = datetime.now().strftime("%Y-%m-%d")
    
    pages = [
        {"loc": "/", "priority": "1.0", "changefreq": "daily"},
        {"loc": "/seasonality", "priority": "0.8", "changefreq": "weekly"},
        {"loc": "/themes", "priority": "0.8", "changefreq": "daily"},
        {"loc": "/leaderboard", "priority": "0.7", "changefreq": "daily"},
        {"loc": "/review", "priority": "0.9", "changefreq": "daily"},
        {"loc": "/policies", "priority": "0.3", "changefreq": "monthly"},
    ]
    
    xml_items = ""
    for p in pages:
        xml_items += f"""  <url>
    <loc>{base_url}{p['loc']}</loc>
    <lastmod>{today}</lastmod>
    <changefreq>{p['changefreq']}</changefreq>
    <priority>{p['priority']}</priority>
  </url>
"""
    
    sitemap_xml = f"""<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
{xml_items}</urlset>"""
    
    return Response(content=sitemap_xml, media_type="application/xml")

@app.get("/robots.txt", response_class=PlainTextResponse)
async def robots_txt():
    """Phase 4: SEO robots.txt for search engine crawlers."""
    return """User-agent: *
Allow: /
Disallow: /api/
Disallow: /portfolio
Sitemap: https://alphafinder.kr/sitemap.xml
"""

# --- 4-4. Dynamic OG Image API ---
@app.get("/api/og-image/{ticker}")
async def generate_og_image(ticker: str, db: Session = Depends(get_db)):
    """
    Phase 4: Generate dynamic Open Graph image data for social sharing.
    Returns JSON with pre-computed OG meta tag values.
    Frontend uses these values in <meta> tags for KakaoTalk/Twitter/Facebook previews.
    """
    resolved_ticker = resolve_ticker(ticker)
    
    # Get latest price data
    try:
        df = fdr.DataReader(resolved_ticker, (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d'))
        if not df.empty:
            current_price = float(df['Close'].iloc[-1])
            price_30d_ago = float(df['Close'].iloc[0])
            change_pct = round((current_price - price_30d_ago) / price_30d_ago * 100, 2)
            if change_pct > 0:
                trend = "ìƒìŠ¹"
            elif change_pct < 0:
                trend = "í•˜ë½"
            else:
                trend = "ë³´í•©"
        else:
            current_price = 0
            change_pct = 0
            trend = "N/A"
    except:
        current_price = 0
        change_pct = 0
        trend = "N/A"
    
    # Vote sentiment
    bull = db.query(models.Vote).filter(models.Vote.ticker == resolved_ticker, models.Vote.vote_type == 'BULL').count()
    bear = db.query(models.Vote).filter(models.Vote.ticker == resolved_ticker, models.Vote.vote_type == 'BEAR').count()
    total = bull + bear
    sentiment = f"BULL {round(bull/total*100)}%" if total > 0 else "íˆ¬í‘œ ì—†ìŒ"
    
    return {
        "title": f"AlphaFinder | {resolved_ticker} ì¢…ëª© AI ë¶„ì„",
        "description": f"{trend} {change_pct:+.2f}% (30ì¼) | í˜„ì¬ê°€ {current_price:,.0f}ì› | íˆ¬ìì ì‹¬ë¦¬: {sentiment}",
        "image_text": f"{resolved_ticker} | {current_price:,.0f}ì› | {change_pct:+.2f}%",
        "ticker": resolved_ticker,
        "current_price": current_price,
        "change_pct": change_pct,
        "sentiment": sentiment
    }

# --- 4-5. P&L (Profit & Loss) Certificate Image Data API ---
@app.get("/api/pnl-card")
def generate_pnl_card(
    db: Session = Depends(get_db),
    current_user: models.User = Depends(auth.get_current_user)
):
    """
    Phase 4: Generate data for a P&L (profit/loss) sharing card.
    Returns structured data that the frontend renders as a shareable image.
    """
    portfolios = db.query(models.Portfolio).filter(
        models.Portfolio.user_id == current_user.id
    ).all()
    
    holdings = []
    total_pnl = 0.0
    
    for p in portfolios:
        try:
            df = fdr.DataReader(p.ticker, (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d'))
            if not df.empty and p.target_price and p.target_price > 0:
                current = float(df['Close'].iloc[-1])
                pnl_pct = round((current - p.target_price) / p.target_price * 100, 2)
                pnl_value = round((current - p.target_price) * p.qty, 0)
                total_pnl += pnl_value
                holdings.append({
                    "ticker": p.ticker,
                    "buy_price": p.target_price,
                    "current_price": current,
                    "qty": p.qty,
                    "pnl_pct": pnl_pct,
                    "pnl_value": pnl_value
                })
        except:
            pass
    
    # Determine rank
    rank_data = db.query(models.User.username, models.User.total_return)\
        .filter(models.User.total_return.isnot(None))\
        .order_by(models.User.total_return.desc()).all()
    
    user_rank = "N/A"
    for i, r in enumerate(rank_data):
        if r.username == current_user.username:
            user_rank = f"{i + 1}/{len(rank_data)}"
            break
    
    return {
        "username": current_user.username,
        "total_return": current_user.total_return or 0.0,
        "rank": user_rank,
        "holdings": holdings,
        "total_pnl_value": total_pnl,
        "generated_at": datetime.now().isoformat(),
        "watermark": "AlphaFinder | alphafinder.kr"
    }

if __name__ == "__main__":
    import uvicorn
    # Make sure to run the app with 'uvicorn main:app --reload' in production
    uvicorn.run(app, host="127.0.0.1", port=8000)
